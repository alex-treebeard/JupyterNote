{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "책 *강화학습첫걸음*\n",
    "\n",
    "# 1. 강화학습의 세 요소\n",
    "\n",
    "- 액션 의존성: 각 액션은 다른 보상을 가져옵니다.\n",
    "- 시간 의존성: 보상은 시간이 지연되고 나서야 주어집니다.\n",
    "- 상태 의존성: 보상은 환경의 상태에 좌우됩니다.\n",
    "\n",
    "# 밴딧 문제\n",
    "강화학습에서 가장 단순한 형태의 문제\n",
    "\n",
    "비용을 수식으로 표시하면 다음과 같습니다.\n",
    "\n",
    "$$ Loss = - log(\\pi)*A $$\n",
    "여기서 A는 **Advantage**로서 강화학습 알고리즘의 필수요소 입니다.\n",
    "\n",
    "# 마르코프 결정 과정(MDP)\n",
    "마르코프 결정과정(Markov Decision process)는 에이전트에게 완전한 문제를 제시하는 환경입니다. 어떤 과제든 MDP로 표현할 수 있습니다. 예를 들어 문을 여는 것을 상상해 봅니다. 이때의 상태는 우리의 몸의 위치와 문의 위치등이 될 수 있습니다. 행동은 우리의 몸이 만들어내는 모든 동작이 됩니다. 보상은 열린 문이 되겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용한 Numpy 버전 1.12.1, Tensorflow 버전은 1.3.0 입니다.\n"
     ]
    }
   ],
   "source": [
    "# 필요한 라이브러리를 불러옵니다.\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"사용한 Numpy 버전 {}, Tensorflow 버전은 {} 입니다.\".format(np.version.version , tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 기본적인 구현\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandit_arms"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
